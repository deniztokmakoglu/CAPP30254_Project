{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deniztokmakoglu/CAPP30254_Project/blob/main/hw4_rnn_lang_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiIVgmEqBBkh"
      },
      "source": [
        "# An RNN Transducer-based Language ModelÂ¶\n",
        "\n",
        "In this homework we will\n",
        "- Build an LSTM transducer-based language model using early stopping and compute the text perplexity.\n",
        "- Use the model to generate sentences.\n",
        "- Extend the model and compare performance when we \n",
        "    - replace the LSTM with a GRU or a Simple RNN\n",
        "    - increase the number of LSTM layers\n",
        "    - add dropout\n",
        "    - add gradient clipping\n",
        "    \n",
        "You can develop on your local machine, but to train on the full training set requires GPUs.  We recommend using the GPUs at [Google Colab](https://colab.research.google.com). To upload a notebook, choose the \"Files\" dropdown menu and then \"Upload.\"  To use a GPU, choose Runtime > Change runtime type and select GPU.    \n",
        "    \n",
        "Acknowledgement:  This assignment was originally written by Zewei Chu, and was inspired by a [homework in CS287](https://github.com/harvard-ml-courses/cs287-s18/blob/master/HW2/Homework%202.ipynb) at Harvard.\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jukjFJVYOEbY"
      },
      "source": [
        "### Using an older version of torchtext\n",
        "\n",
        "Torchtext is undergoing rapid development.  The latest version of the library has dropped some components, which are expected to be revamped and added back in the future.  So for this homework, we'll have to work with a slightly older version, 0.11.2.  Please use the following command to install the correct version, if your version is different.\n",
        "\n",
        "`!pip install torchtext==0.11.2`\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install torchtext==0.11.2"
      ],
      "metadata": {
        "id": "6das5efhOWZ9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvwWVsq5OEbZ",
        "outputId": "c1eaf4b3-4020-4510-b368-238ede5e0da9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1.10.2+cu102', '0.11.2')"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "# Check your version\n",
        "import torch\n",
        "import torchtext\n",
        "# On Colab, you'll see ('1.10.2+cu102', '0.11.2')\n",
        "torch.__version__, torchtext.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgvxDyBaOEba"
      },
      "source": [
        "### Development vs full version\n",
        "\n",
        "Choose the appropriate version of the parameters using the switches `DEVELOPING` and `COLAB.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvGYWOeMBBkj",
        "outputId": "8560644a-9d9a-4071-d622-7736b2f0b5dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda.\n",
            "Full version\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torchtext\n",
        "from torchtext.vocab import Vectors\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "\n",
        "if USE_CUDA:\n",
        "    DEVICE = torch.device('cuda')\n",
        "    print(\"Using cuda.\")\n",
        "else:\n",
        "    DEVICE = torch.device('cpu')\n",
        "    print(\"Using cpu.\")\n",
        "\n",
        "random.seed(30255)\n",
        "np.random.seed(30255)\n",
        "torch.manual_seed(30255)\n",
        "if USE_CUDA:\n",
        "    torch.cuda.manual_seed(30255)\n",
        "\n",
        "# Change the following to false when training on\n",
        "# the full set\n",
        "#DEVELOPING = True    \n",
        "DEVELOPING = False\n",
        "\n",
        "if DEVELOPING:\n",
        "    print('Small development version')\n",
        "    BATCH_SIZE = 4\n",
        "    EMBEDDING_SIZE = 20\n",
        "    MAX_VOCAB_SIZE = 5000\n",
        "    TRAIN_DATA_SET = \"lm-train-small.txt\"\n",
        "    DEV_DATA_SET = \"lm-dev-small.txt\"\n",
        "    TEST_DATA_SET = \"lm-test-small.txt\"\n",
        "    BPTT_LENGTH = 8\n",
        "else:\n",
        "    print('Full version')\n",
        "    BATCH_SIZE = 32\n",
        "    EMBEDDING_SIZE = 650\n",
        "    MAX_VOCAB_SIZE = 50000\n",
        "    TRAIN_DATA_SET = \"lm-train.txt\"\n",
        "    DEV_DATA_SET = \"lm-dev.txt\"\n",
        "    TEST_DATA_SET = \"lm-test.txt\"\n",
        "    BPTT_LENGTH = 32\n",
        "\n",
        "# For uploading data to Colab see, e.g., \n",
        "# https://medium.com/@philipplies/transferring-data-from-google-drive-to-google-cloud-storage-using-google-colab-96e088a8c041    \n",
        "#COLAB = False\n",
        "COLAB = True\n",
        "if COLAB:\n",
        "    from google.colab import drive \n",
        "    drive.mount('/content/gdrive')\n",
        "    PATH = \"gdrive/My Drive/\"\n",
        "else:\n",
        "    PATH = \"/Users/amitabh/mlpp22/Homework/hw4/\"\n",
        "    \n",
        "    \n",
        "LOG_FILE = \"language-model.log\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQIs9OK2BBkj"
      },
      "source": [
        "### Preprocessing using the legacy component of TorchText\n",
        "\n",
        "For our preprocessing we'll use the legacy component in TorchText version 0.11.2.  [Documentation](https://torchtext.readthedocs.io/en/latest/index.html) for this legacy component torchtext is relatively sparse (and, unfortunately, not very clear), but [Ben Trevett](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb) has a useful tutorial.  (If you are keen to understand this component, you may also want to look at the [source code](https://github.com/pytorch/text/tree/master/torchtext/legacy).)\n",
        "\n",
        "All the **legacy torchtext code is already provided**. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe2FRmolBBkn",
        "outputId": "98d88ac4-1641-4df8-eb70-624e8cdb64e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 50002\n"
          ]
        }
      ],
      "source": [
        "TEXT = torchtext.legacy.data.Field(lower=True)\n",
        "\n",
        "train, val, test = torchtext.legacy.datasets.LanguageModelingDataset.splits(path=PATH, \n",
        "    train=TRAIN_DATA_SET, validation=DEV_DATA_SET, test=TEST_DATA_SET, text_field=TEXT)\n",
        "\n",
        "TEXT.build_vocab(train, max_size=MAX_VOCAB_SIZE)\n",
        "VOCAB_SIZE = len(TEXT.vocab)\n",
        "\n",
        "print(f'Vocabulary size: {VOCAB_SIZE}')\n",
        "\n",
        "train_iter, val_iter, test_iter = torchtext.legacy.data.BPTTIterator.splits(\n",
        "    (train, val, test), batch_size=BATCH_SIZE, device=DEVICE, bptt_len=BPTT_LENGTH, \n",
        "    repeat=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y7Us1voBBkr"
      },
      "source": [
        "### Back propagation through time (BPTT) iterator\n",
        "\n",
        "The [BPTTIterator](https://torchtext.readthedocs.io/en/latest/data.html#bpttiterator) is a custom torchtext iterator for language modeling using RNNs.  Suppose the text in an example is \"the quick brown fox\".  The target in the transducer-based RNN language model would then be \"quick brown fox jumps\".  This allows every prefix of the text to be used as an training example, with the corresponding word in the target text as the target word.  So the above would lead to four examples, written as text sequence -> target word:\n",
        "* \"the\" -> \"quick\"\n",
        "* \"the quick\" -> \"brown\"\n",
        "* \"the quick brown\" -> \"fox\"\n",
        "* \"the quick brown fox\" -> \"jump\"\n",
        "\n",
        "(Unlike some of the examples in class, here we treat words as part of a sequence without special consideration for sentences.  In particular, we don't use start/end of setence tags.)\n",
        "\n",
        "One very **significant feature** of the BPTTIterator is that examples continue across batches.  To illustrate let the original data be one long seqence $w_1, w_2, \\ldots, w_N$, in which, say, $N = 4,000$.  Further let each batch consist of $4$ examples, each of length 8.  Then the first batch created by BPTTIterator would be the following 4 examples---\n",
        "\n",
        "- $(w_1, w_2, \\ldots, w_{8}), (w_{1001}, w_{1002}, \\ldots, w_{1008}), \\ldots, (w_{3001}, w_{3002}, \\ldots, w_{3008}).$ \n",
        "\n",
        "and the second batch would be---\n",
        "\n",
        "- $(w_{9}, w_{10}, \\ldots, w_{16}), (w_{1009}, w_{1010}, \\ldots, w_{1016}), \\ldots, (w_{3009}, w_{3010}, \\ldots, w_{3016}).$\n",
        "\n",
        "This has implications on how the hidden state of the RNN is set for the second batch onwards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqBOF32nBBks",
        "outputId": "04475c5f-a775-460c-8695-4cf17f28e496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first three text/target sequences from the first batch are:\n",
            "\n",
            "     Text Sequence 0: anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans <unk> of the french revolution whilst the term\n",
            "     Target Sequence 0: originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans <unk> of the french revolution whilst the term is\n",
            "\n",
            "     Text Sequence 1: of natural history albert einstein the albert einstein institution the economist one zero zero years of einstein einstein home distributed computing project searching for gravitational waves predicted by einstein s theories world\n",
            "     Target Sequence 1: natural history albert einstein the albert einstein institution the economist one zero zero years of einstein einstein home distributed computing project searching for gravitational waves predicted by einstein s theories world year\n",
            "\n",
            "     Text Sequence 2: one eight eight two he set up a practice in plymouth he achieved his doctorate in one eight eight five his medical practice was unsuccessful while waiting for patients he began writing\n",
            "     Target Sequence 2: eight eight two he set up a practice in plymouth he achieved his doctorate in one eight eight five his medical practice was unsuccessful while waiting for patients he began writing stories\n",
            "\n",
            "Each sequence has BPTT_LENGTH = 32.\n",
            "\n",
            "Also the sequences continue in the next batch!\n",
            "\n",
            "     Text Sequence 0: is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by\n",
            "     Target Sequence 0: still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self\n",
            "\n",
            "     Text Sequence 1: year of physics two zero zero five a celebration of einstein s miracle year einstein year two zero zero five the guardian einstein s pacifist dilemma revealed einstein s theory of relativity\n",
            "     Target Sequence 1: of physics two zero zero five a celebration of einstein s miracle year einstein year two zero zero five the guardian einstein s pacifist dilemma revealed einstein s theory of relativity in\n",
            "\n",
            "     Text Sequence 2: stories his first literary experience came in chambers s edinburgh journal before he was two zero it was only after he subsequently moved his practice to <unk> that he began to indulge\n",
            "     Target Sequence 2: his first literary experience came in chambers s edinburgh journal before he was two zero it was only after he subsequently moved his practice to <unk> that he began to indulge more\n",
            "\n"
          ]
        }
      ],
      "source": [
        "it = iter(train_iter)\n",
        "batch = next(it)\n",
        "print(\"The first three text/target sequences from the first batch are:\\n\")\n",
        "indent = \" \" * 4\n",
        "for j in range(3):\n",
        "    print(indent, f\"Text Sequence {j}:\", \n",
        "          \" \".join([TEXT.vocab.itos[i] for i in batch.text[:,j].data]))\n",
        "    print(indent, f\"Target Sequence {j}:\",\n",
        "          \" \".join([TEXT.vocab.itos[i] for i in batch.target[:,j].data]))\n",
        "    print()\n",
        " \n",
        "print(f\"Each sequence has BPTT_LENGTH = {BPTT_LENGTH}.\\n\")\n",
        "print(\"Also the sequences continue in the next batch!\\n\")\n",
        "batch = next(it)\n",
        "for j in range(3):\n",
        "    print(indent, f\"Text Sequence {j}:\", \n",
        "          \" \".join([TEXT.vocab.itos[i] for i in batch.text[:,j].data]))\n",
        "    print(indent, f\"Target Sequence {j}:\",\n",
        "          \" \".join([TEXT.vocab.itos[i] for i in batch.target[:,j].data]))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smzi0zXzBBkv"
      },
      "source": [
        "#### Initializing hidden vectors from the detached hidden vectors of previous batch\n",
        "\n",
        "Since sequences continue across batches, for proper training, **the final output hidden vectors in a batch should be used to initialize the hidden vectors for the next batch**.  But care should be taken to detach vectors used for initialization from the computational graph, else gradients would flow \"from one batch to the previous\" and training would be increasingly slow. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B853z05BBkv"
      },
      "source": [
        "### Define the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uge8v-wXj4Q"
      },
      "source": [
        "\n",
        "Our RNN based language model (when using an LSTM) for a language model is as follows:\n",
        "- Let the input sequence---the *context*---be $w_1, w_2, \\ldots, w_n$, and let the target sequence be $w_2, \\ldots, w_n, w_{n+1}$.\n",
        "- At step $i$ of the input, for $1 \\leq i \\leq n$:\n",
        "    - $x_i = E_{[w_i]}$.\n",
        "    - $y_i, (h_i, c_i) = \\text{LSTM}(x_i, (h_{i-1}, c_{i-1}))$.  For LSTMs, $y_i$ equals $h_i$.\n",
        "    - $\\widehat{y_i} = \\text{softmax}(y_i W + b)$ in which $\\widehat{y_i}$ is the predicted probability distribution for $w_{i+1}$.\n",
        "    - In the above \n",
        "        - $x_i$ is $1 \\times \\text{embedding dim}$ \n",
        "        - $y_i$, $h_i$ and $c_i$ are $1 \\times \\text{hidden dim}$\n",
        "        - $\\widehat{y}_i$ is $1 \\times \\text{vocab size}$.\n",
        "- The loss $\\ell = \\sum_{i=1}^n \\log \\widehat{y}_{i_{[w_{i+1}]}}$, in which $\\log \\widehat{y}_{i_{[w_{i+1}]}}$ is the component of $\\log \\widehat{y}_{i}$ corresponding to the element $w_{i+1}$.\n",
        "\n",
        "Since the sequences continue across batches we retain the hidden states across batches. Specifically, consider the $k$th example in batch $j$.  For $j=1$, i.e., first batch, the corresponding $(h_0, c_0)$ for the $k$th example is set to all zeros.  But for $j > 1$, the corresponding $(h_0, c_0)$ is set to $(h_{n}, c_{n})$ of the $k$th example in batch $j-1$.\n",
        "\n",
        "In PyTorch we do not call the forward function separately for each step $i$.  Instead we call the model with\n",
        "\n",
        "- tensors corresponding to $(w_1, w_2, \\ldots, w_n)$ and $(h_0, c_0)$\n",
        "\n",
        "and receive as ouput\n",
        "\n",
        "- $(y_1, y_2, \\ldots, y_n)$ and $(h_n, c_n)$.\n",
        "\n",
        "Further the above is combined for several examples into one batch.  Please read the PyTorch documentation to learn more about building models\n",
        "            with RNNs.  E.g., see the documentation on [LSTMs](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) and [Robert Gutherie's Tutorial](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#sphx-glr-beginner-nlp-sequence-models-tutorial-py) on working with LSTMs.\n",
        "            \n",
        "The above can be adapted easily to [GRUs](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html) or [Simple RNNs](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) since the PyTorch interface is very similar.\n",
        "\n",
        "**Task 1** [20 points]: Complete the code for the class `RNNLM` based on the description above.  (Some extra parameters are provided since in a later task, you'll modify your code to incorporate the following: (i) replace the LSTM with a GRU or a Simple RNN, (ii) increase the number of LSTM layers, and (iii) add dropout.) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "beflzeEkBBkw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class RNNLM(nn.Module):\n",
        "    \"\"\" Container module with an linear encoder/embedding, an RNN module, and a linear decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, rnn_type, vocab_size, embedding_dim, hidden_dim, num_layers, \n",
        "                 dropout=0.5):\n",
        "        ''' Initialize model parameters corresponding to ---\n",
        "            - embedding layer\n",
        "            - recurrent neural network layer---one of LSTM, GRU, or RNN---with \n",
        "              optionally more than one layer\n",
        "            - linear layer to map from hidden vector to the vocabulary\n",
        "            - optionally, dropout layers.  Dropout layers can be placed after \n",
        "              the embedding layer or/and after the RNN layer. Dropout within\n",
        "              an RNN is only applied when there are two or more num_layers.\n",
        "            - optionally, initialize the model parameters.\n",
        "            \n",
        "            The arguments are:\n",
        "            \n",
        "            rnn_type: One of 'LSTM', 'GRU', 'RNN_TANH', 'RNN_RELU'\n",
        "            vocab_size: size of vocabulary\n",
        "            embedding_dim: size of an embedding vector\n",
        "            hidden_dim: size of hidden/state vector in RNN\n",
        "            num_layers: number of layers in RNN\n",
        "            dropout: dropout probability.\n",
        "            \n",
        "        '''\n",
        "        super(RNNLM, self).__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        if rnn_type == \"LSTM\":\n",
        "          self.lstm = nn.LSTM(input_size=embedding_dim,\n",
        "                            hidden_size=hidden_dim,\n",
        "                            num_layers = num_layers)\n",
        "        elif rnn_type == \"GRU\":\n",
        "          self.GRU = nn.GRU(input_size=embedding_dim,\n",
        "                            hidden_size=hidden_dim,\n",
        "                            num_layers = num_layers)\n",
        "        elif rnn_type == \"RNN_TANH\":\n",
        "          self.RNN_TANH = nn.RNN(input_size=embedding_dim,\n",
        "                            hidden_size=hidden_dim,\n",
        "                            num_layers = num_layers,\n",
        "                            nonlinearity = \"tanh\")\n",
        "        elif rnn_type == \"RNN_RELU\":\n",
        "          self.RNN_RELU =nn.RNN(input_size=embedding_dim,\n",
        "                             hidden_size=hidden_dim,\n",
        "                            num_layers = num_layers,\n",
        "                            nonlinearity = \"relu\")\n",
        "        else:\n",
        "          assert \"Incorrect RNN Type.\"\n",
        "        \n",
        "        self.dropout = nn.Dropout(p = dropout)\n",
        "        self.decoder = nn.Linear(hidden_dim, vocab_size) #vocab_size?\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.n_layers = num_layers\n",
        "        self.nhid = hidden_dim\n",
        "\n",
        "    def forward(self, input, hidden0):\n",
        "        ''' \n",
        "        Run forward propagation for a given minibatch of inputs using\n",
        "        hidden0 as the initial hidden state.\n",
        "\n",
        "        In LSTMs hidden0 = (h_0, c_0). \n",
        "\n",
        "        The output of the RNN includes the hidden vector hiddenn = (h_n, c_n).\n",
        "        Return this as well so that it can be used to initialize the next\n",
        "        batch.\n",
        "        \n",
        "        Unlike previous homework sets do not apply softmax or logsoftmax here, since we'll use\n",
        "        the more efficient CrossEntropyLoss.  See \n",
        "        https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html.\n",
        "        '''\n",
        "        encoded = self.embedding(input.long())\n",
        "        encoded = self.dropout(encoded)\n",
        "        lstm_out, hidden = self.lstm(encoded, hidden0)\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "        decoded = self.decoder(lstm_out)\n",
        "        decoded = torch.squeeze(decoded)\n",
        "\n",
        "        \n",
        "        return decoded.view(BPTT_LENGTH, self.vocab_size, -1), hidden\n",
        "        \n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        return (weight.new_zeros(self.n_layers, bsz, self.nhid),\n",
        "            weight.new_zeros(self.n_layers, bsz, self.nhid))\n",
        "        \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOu67_BOBBky"
      },
      "source": [
        "### Evaluate on a given data set\n",
        "\n",
        "The function for evaluation is provided below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "Po5NsbTOBBkz"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data):\n",
        "    '''\n",
        "    Evaluate the model on the given data.\n",
        "    '''\n",
        "  \n",
        "    model.eval()\n",
        "    it = iter(data)\n",
        "    total_count = 0. # Number of target words seen\n",
        "    total_loss = 0. # Loss over all target words\n",
        "    with torch.no_grad():\n",
        "        # No gradients need to be maintained during evaluation\n",
        "        # There are no hidden tensors for the first batch, and so will default to zeros.\n",
        "        hidden = None \n",
        "        for i, batch in enumerate(it):\n",
        "            ''' Do the following:\n",
        "                - Extract the text and target from the batch, and if using CUDA (essentially, using GPUs), place \n",
        "                  the tensors on cuda, using a commands such as \"text = text.cuda()\".  More details are at\n",
        "                  https://pytorch.org/docs/stable/notes/cuda.html.\n",
        "                - Pass the hidden state vector from output of previous batch as the initial hidden vector for\n",
        "                  the current batch. \n",
        "                - Call forward propagation to get output and final hidden state vector.\n",
        "                - Compute the cross entropy loss\n",
        "                - The loss_fn computes the average loss per target word in the batch.  Count the number of target\n",
        "                  words in the batch (it is usually the same, except for the last batch), and use it to track the \n",
        "                  total count (of target words) and total loss see so far over all batches.\n",
        "            '''\n",
        "            text, target = batch.text, batch.target\n",
        "            if USE_CUDA:\n",
        "                text, target = text.cuda(), target.cuda()\n",
        "            if target.shape[0] != 10: #kicking out the last batch\n",
        "              output, hidden = model(text, hidden)\n",
        "              loss = loss_fn(output, target)\n",
        "              total_count += np.multiply(*text.size())\n",
        "              total_loss += loss.item()*np.multiply(*text.size())\n",
        "                \n",
        "    loss = total_loss / total_count\n",
        "    model.train()\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcg9tUbNOEbg"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Training the model is mostly similar to previous homework sets except for:\n",
        "- A detached hidden vector is applied to the second batch onwards as described above.\n",
        "- Every, say, 10,000 iterations evaluate the model on a validation set, and if the mean loss is the lowest so far, save a copy of it.  After training, this \"best model\" is used for testing. \n",
        "      \n",
        "**Task 2** [30]: Complete the code below for training the model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, data, i, hidden = None, USE_CUDA = True, print_every = 250):\n",
        "  start_time = time.time()\n",
        "  total_loss = 0\n",
        "  text, target = data.text, data.target\n",
        "  if USE_CUDA:\n",
        "      text, target = text.cuda(), target.cuda()\n",
        "  hidden = repackage_hidden(hidden)\n",
        "  output, hidden = model(text, hidden)\n",
        "\n",
        "  if output.shape[0] == target.shape[0]:\n",
        "    loss = loss_fn(output, target.long())\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "  if (i + 1) % print_every == 0:\n",
        "      cur_loss = total_loss / print_every\n",
        "      elapsed = time.time() - start_time\n",
        "      print(f\"Average loss so far: {round(cur_loss, 2)} | Batch {i+1} | Time {round(elapsed, 2)}\")\n",
        "      total_loss = 0\n",
        "      start_time = time.time()"
      ],
      "metadata": {
        "id": "Vk_JyF-WvDcR"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "8dlLJ5FTBBk1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "188cf0db-951d-4271-8abe-45a1bc666989"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " #### EPOCH 0 #####\n",
            " #### EPOCH 1 #####\n"
          ]
        }
      ],
      "source": [
        "GRAD_CLIP = 1.\n",
        "NUM_EPOCHS = 2\n",
        "LOG_INTERVAL = 100\n",
        "import os\n",
        "SAVE_BEST = os.path.join(PATH, 'model.pt')\n",
        "import time\n",
        "import math\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if h is None:\n",
        "        return None\n",
        "    elif isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "model = RNNLM(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, dropout=0.5)\n",
        "if USE_CUDA:\n",
        "    model = model.cuda()\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss() ## Used instead of NLLLoss.\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "val_losses = []\n",
        "best_model = None\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\"\" #### EPOCH {epoch} #####\"\"\")\n",
        "    model.train()\n",
        "    # There are no hidden tensors for the first batch, and so will default to zeros.\n",
        "    hidden = None\n",
        "    min_val_loss = math.inf\n",
        "    best_model = None\n",
        "    for i, batch in enumerate(it):\n",
        "      train_model(model, batch, i)\n",
        "  \n",
        "      if (i + 1) % 1000 == 0:\n",
        "        val_loss = evaluate(model, val_iter)\n",
        "        \n",
        "        val_losses.append(val_loss.item())\n",
        "        min_val_loss = min(min_val_loss, val_loss.item())\n",
        "        print(f\"Average val loss {sum(val_losses) / len(val_losses)}\")\n",
        "        if min_val_loss == val_loss.item():\n",
        "          with open(SAVE_BEST, 'wb') as f:\n",
        "            torch.save(model, f)\n",
        "          print(\"New best model saved!\")\n",
        "\n",
        "        \n",
        "       \n",
        "\n",
        "    ''' Do the following:\n",
        "      \n",
        "        - Pass the hidden state vector from output of previous batch as the initial hidden vector for\n",
        "          the current batch. But detach each tensor in the hidden state vector using tensor.detach() or\n",
        "          the provided repackage_hidden(). See\n",
        "          https://pytorch.org/docs/master/generated/torch.Tensor.detach_.html#torch-tensor-detach\n",
        "        - Zero out the model gradients to reset backpropagation for current batch\n",
        "        - Call forward propagation to get output and final hidden state vector.\n",
        "        - Compute the cross entropy loss\n",
        "        - Run back propagation to set the gradients for each model parameter.\n",
        "        - Clip the gradients that may have exploded. See Sec 5.2.4 in the Goldberg textbook, and\n",
        "          https://pytorch.org/docs/master/generated/torch.nn.utils.clip_grad_norm_.html#torch-nn-utils-clip-grad-norm\n",
        "        - Run a step of gradient descent. \n",
        "        - Print the batch loss after every few iterations. (Say every 100 when developing, every 1000 otherwise.)\n",
        "        - Evaluate your model on the validation set after every, say, 10000 iterations and save it to val_losses. If\n",
        "          your model has the lowest validation loss so far, copy it to best_model. For that it is recommended that\n",
        "          copy the state_dict rather than use deepcopy, since the latter doesn't work on Colab.  See discussion at \n",
        "          https://discuss.pytorch.org/t/deep-copying-pytorch-modules/13514. This is Early Stopping and is described\n",
        "          in Sec 2.3.1 of Lecture notes by Cho: \n",
        "          https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf\n",
        "    '''\n",
        "    \n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(SAVE_BEST, 'rb') as f:\n",
        "    best_model = torch.load(f)\n",
        "    # After loading the RNN params, they are not a continuous chunk of memory.\n",
        "    # flatten_paramters() makes them a continuous chunk, and will speed up the forward pass.\n",
        "    # Currently, only RNN model supports flatten_parameters function.\n",
        "    best_model.lstm.flatten_parameters()\n"
      ],
      "metadata": {
        "id": "3T6KhW2cGVuP"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "x3ooK74UBBk4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4560a27a-3490-40f4-b960-ac409e93b97c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity:  16812.515798108216\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Evaluate the loss of best_model on the validation set and compute its perplexity.\n",
        "'''\n",
        "## load best model\n",
        "with open(SAVE_BEST, 'rb') as f:\n",
        "    best_model = torch.load(f)\n",
        "    # After loading the RNN params, they are not a continuous chunk of memory.\n",
        "    # flatten_paramters() makes them a continuous chunk, and will speed up the forward pass.\n",
        "    # Currently, only RNN model supports flatten_parameters function.\n",
        "    best_model.lstm.flatten_parameters()\n",
        "\n",
        "best_model_set = RNNLM(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, dropout=0.5)\n",
        "\n",
        "\n",
        "val_loss = evaluate(best_model, val_iter)\n",
        "print(\"perplexity: \", np.exp(val_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6klWxMwBBk7"
      },
      "source": [
        "### Use the best model to evaluate the test dataset. \n",
        "\n",
        "We expect a test perplexity of less than 250 on the full model after a couple of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "oE7CK7XxBBk7",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1c9bccb-cda9-48e0-e7a4-8413fb8477cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity:  16190.229473819982\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Evaluate the loss of best_model on the test set and compute its perplexity.\n",
        "'''\n",
        "test_loss = evaluate(best_model, test_iter)\n",
        "print(\"perplexity: \", np.exp(test_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ7-S1FuBBk9"
      },
      "source": [
        "### Use the model to generate some sentences\n",
        "\n",
        "**Task 3** [20]: Write code to generate random sentences.  Section 9.5 in the Goldberg textbook describes how this can be done.  Since we don't have a start symbol, for the first word simply pick a random word from the vocabulary.\n",
        "\n",
        "You'll notice that the full sequences don't make much sense, but subsequences sound reasonably correct. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "LNSlzc-8BBk-"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Use the model to generate 5 random sequences of length 50 each.\n",
        "'''\n",
        "def generate_text(sampling_func, model = best_model):\n",
        "    # # Generation with LSTM lm given a sampling function and a prompt\n",
        "    prompt = random.choice(TEXT.vocab.itos)\n",
        "    id_word = TEXT.vocab.itos.index(prompt)\n",
        "    max_length = 50\n",
        "    hidden = model.init_hidden(1)\n",
        "    hidden = repackage_hidden(hidden)\n",
        "    with torch.no_grad():  # no tracking history\n",
        "        input = torch.tensor(id_word).to(\"cuda\")\n",
        "        output, hidden = model(torch.tensor([[id_word] * 32]).to(\"cuda\"), None)\n",
        "        word_prob = torch.nn.functional.softmax(output[-1,:], dim=0).cpu()\n",
        "        generations = []\n",
        "        for i in range(max_length):\n",
        "            word_idx = sampling_func(word_prob)\n",
        "            word = TEXT.vocab.itos[word_idx]\n",
        "            generations.append(word)\n",
        "            if word == \"<eos>\":\n",
        "                break\n",
        "            new_word = torch.LongTensor([[word_idx] * 32]).to(\"cuda\")\n",
        "            output, hidden = model(new_word, None)\n",
        "            word_prob = torch.nn.functional.softmax(output[-1,:], dim=0).cpu()\n",
        "    return generations\n",
        "\n",
        "def topk_sampling_5(word_prob):\n",
        "    k = 10\n",
        "    topk = torch.topk(word_prob.flatten(), k)\n",
        "    values = topk.values / topk.values.sum()\n",
        "    indices = topk.indices\n",
        "    index = torch.multinomial(values, 1).item()\n",
        "    word_id = list(indices)[index]\n",
        "    return word_id"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  generations = generate_text(topk_sampling_5) # replace sample_func with the sampling function that you would like to try\n",
        "  print('prompt: ' + \" \".join(generations))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdJVWKsAuJ9Y",
        "outputId": "ee43ef4e-55e5-4b79-b7ce-932f160001e8"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: would people so so so people may called if th called often after into may after if however would may if called people would so may i if if if war would so so so only if people after people will would will south i if i d so however\n",
            "prompt: known so through so i may if i so however b called during will so people may may may so however b hermitian b article called people would would through i may would b may will people called b asparagine system people d i so d where would may b\n",
            "prompt: d article called b nl after called would b vitrification b scale so may called d however may however will however may would would called d see known if may if would during b aau if would often b states will so called called however after may after i b\n",
            "prompt: however may would south would united will may people would called if called so b called people so so so so over if may would i people b external if so will will than people may after so would would called i so called only people would so people b\n",
            "prompt: b shelves may so so if so d may called b vitrification if d people may would would if often b fruitless century would called would war however often world may may called see known b known will people b cerberus so people may states known called b cerberus people\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEqN7I4xOEbi"
      },
      "source": [
        "### Choose the best sentence from alternatives\n",
        "\n",
        "Generating random sentences as above is, however, not the objective of a language model.  Rather it is used as an auxiliary tool to choose the best sequence given some choices by comparing their perplexities.\n",
        "\n",
        "**Task 4** [5]: Use the code below to compute perplexities of the given six sentences.  Discuss the model's performance in choosing the best alternative.  (The code uses TorchText functions which are designed for much larger datasets.  So the perplexities below are approximate. Even so they illustrate the usefullness of our model.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "UvIWKlt1OEbj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "1f90d442-896d-4a0c-866f-47701a6ec995"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early in the pandemic, there was hope that the world would one day achieve herd immunity, the point when the coronavirus lacks hosts to spread easily. But over a year later, the virus is crushing India with a fearsome second wave and surging in countries from Asia to Latin America.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-196-b8778d242c33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m                                                   bptt_len=BPTT_LENGTH, repeat=False)\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0msen_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msen_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"perplexity: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-131-4e86d69007fb>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#kicking out the last batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m               \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m               \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m               \u001b[0mtotal_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m               \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (32) to match target batch_size (1)."
          ]
        }
      ],
      "source": [
        "sen1 = (\"Early in the pandemic, there was hope that the world would one day achieve herd immunity, \"\n",
        "\"the point when the coronavirus lacks hosts to spread easily. But over a year later, the virus is \" \n",
        "\"crushing \"\n",
        "\"India with a fearsome second wave and surging in countries from Asia to Latin America.\")\n",
        "\n",
        "sen2 = (\"Early in the pandemic, there was hope that the world would one day achieve herd immunity, \"\n",
        "\"the point when the coronavirus lacks hosts to spread easily. But over a year later, the virus is \"\n",
        "\"dancing \"\n",
        "\"India with a fearsome second wave and surging in countries from Asia to Latin America.\")\n",
        "\n",
        "sen3 = (\"Early in the pandemic, there was hope that the world would one day achieve herd immunity, \"\n",
        "\"the point when the coronavirus lacks hosts to spread easily. But over a year later, the virus is \" \n",
        "\"run \"\n",
        "\"India with a fearsome second wave and surging in countries from Asia to Latin America.\")\n",
        "\n",
        "sen4 = (\"Early in the pandemic, there was hope that the \"\n",
        "\"cat \"\n",
        "\" would one day achieve herd immunity, \"\n",
        "\"the point when the coronavirus lacks hosts to spread easily. But over a year later, the virus is\"\n",
        "\"run \"\n",
        "\"India with a fearsome second wave and surging in countries from Asia to Latin America.\")\n",
        "\n",
        "sen5 = sen1.split()\n",
        "random.shuffle(sen5)\n",
        "sen5 = \" \".join(sen5)\n",
        "\n",
        "sen6 = \" \".join(['Early in the pandemic']*8)\n",
        "\n",
        "sen_list = [sen1, sen2, sen3, sen4, sen5, sen6]\n",
        "\n",
        "for sen in sen_list:\n",
        "\n",
        "    print(sen)\n",
        "    with open(PATH + \"temp_sentence.txt\", 'w') as text_file:\n",
        "        print(sen, file = text_file)\n",
        "\n",
        "    temp_ds = torchtext.legacy.datasets.LanguageModelingDataset(path=PATH + 'temp_sentence.txt', \n",
        "                                                                text_field=TEXT)\n",
        "\n",
        "\n",
        "    sen_iter = torchtext.legacy.data.BPTTIterator(temp_ds, batch_size=BATCH_SIZE, device=DEVICE, \n",
        "                                                  bptt_len=BPTT_LENGTH, repeat=False)\n",
        "        \n",
        "    sen_loss = evaluate(best_model, sen_iter)\n",
        "    print(\"perplexity: \", np.exp(sen_loss))\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sen_iter"
      ],
      "metadata": {
        "id": "W5tXzW1OLLZx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SzfKRtMBBlA"
      },
      "source": [
        "### Extensions\n",
        "\n",
        "**Task 5** [25]: Extend your model to incorporate the following options: (i) substitute the LSTM with a GRU or a Simple RNN, (ii) increase the number of LSTM layers, (iii) add dropout, (iv) add gradient clipping.  Report on the combination of these options which gives the best performance."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqgsHnQiOEbj"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "hw4_rnn_lang_model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}