{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deniztokmakoglu/CAPP30254_Project/blob/main/hw4_rnn_lang_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiIVgmEqBBkh"
      },
      "source": [
        "# An RNN Transducer-based Language Model¶\n",
        "\n",
        "In this homework we will\n",
        "- Build an LSTM transducer-based language model using early stopping and compute the text perplexity.\n",
        "- Use the model to generate sentences.\n",
        "- Extend the model and compare performance when we \n",
        "    - replace the LSTM with a GRU or a Simple RNN\n",
        "    - increase the number of LSTM layers\n",
        "    - add dropout\n",
        "    - add gradient clipping\n",
        "    \n",
        "You can develop on your local machine, but to train on the full training set requires GPUs.  We recommend using the GPUs at [Google Colab](https://colab.research.google.com). To upload a notebook, choose the \"Files\" dropdown menu and then \"Upload.\"  To use a GPU, choose Runtime > Change runtime type and select GPU.    \n",
        "    \n",
        "Acknowledgement:  This assignment was originally written by Zewei Chu, and was inspired by a [homework in CS287](https://github.com/harvard-ml-courses/cs287-s18/blob/master/HW2/Homework%202.ipynb) at Harvard.\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jukjFJVYOEbY"
      },
      "source": [
        "### Using an older version of torchtext\n",
        "\n",
        "Torchtext is undergoing rapid development.  The latest version of the library has dropped some components, which are expected to be revamped and added back in the future.  So for this homework, we'll have to work with a slightly older version, 0.11.2.  Please use the following command to install the correct version, if your version is different.\n",
        "\n",
        "`!pip install torchtext==0.11.2`\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install torchtext==0.11.2"
      ],
      "metadata": {
        "id": "6das5efhOWZ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a1037f4-1928-46ee-9ce4-e9e7503e07dd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.11.2\n",
            "  Downloading torchtext-0.11.2-cp37-cp37m-manylinux1_x86_64.whl (8.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 1.0 MB/s \n",
            "\u001b[?25hCollecting torch==1.10.2\n",
            "  Downloading torch-1.10.2-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.2 MB/s eta 0:00:41tcmalloc: large alloc 1147494400 bytes == 0x3a5d4000 @  0x7f0dca78a615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████████████████| 881.9 MB 1.9 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.2) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.2) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.2) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.2->torchtext==0.11.2) (4.2.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.2) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.2) (1.24.3)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.12.0\n",
            "    Uninstalling torchtext-0.12.0:\n",
            "      Successfully uninstalled torchtext-0.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.10.2 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.10.2 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.10.2 torchtext-0.11.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvwWVsq5OEbZ",
        "outputId": "2099df07-f69c-40c7-b3e4-55f9e6d60542"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1.10.2+cu102', '0.11.2')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Check your version\n",
        "import torch\n",
        "import torchtext\n",
        "# On Colab, you'll see ('1.10.2+cu102', '0.11.2')\n",
        "torch.__version__, torchtext.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgvxDyBaOEba"
      },
      "source": [
        "### Development vs full version\n",
        "\n",
        "Choose the appropriate version of the parameters using the switches `DEVELOPING` and `COLAB.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvGYWOeMBBkj",
        "outputId": "9363ee1f-89f0-4e9e-d69d-f5add8ce0deb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda.\n",
            "Full version\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torchtext\n",
        "from torchtext.vocab import Vectors\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "\n",
        "if USE_CUDA:\n",
        "    DEVICE = torch.device('cuda')\n",
        "    print(\"Using cuda.\")\n",
        "else:\n",
        "    DEVICE = torch.device('cpu')\n",
        "    print(\"Using cpu.\")\n",
        "\n",
        "random.seed(30255)\n",
        "np.random.seed(30255)\n",
        "torch.manual_seed(30255)\n",
        "if USE_CUDA:\n",
        "    torch.cuda.manual_seed(30255)\n",
        "\n",
        "# Change the following to false when training on\n",
        "# the full set\n",
        "#DEVELOPING = True    \n",
        "DEVELOPING = False\n",
        "\n",
        "if DEVELOPING:\n",
        "    print('Small development version')\n",
        "    BATCH_SIZE = 4\n",
        "    EMBEDDING_SIZE = 20\n",
        "    MAX_VOCAB_SIZE = 5000\n",
        "    TRAIN_DATA_SET = \"lm-train-small.txt\"\n",
        "    DEV_DATA_SET = \"lm-dev-small.txt\"\n",
        "    TEST_DATA_SET = \"lm-test-small.txt\"\n",
        "    BPTT_LENGTH = 8\n",
        "else:\n",
        "    print('Full version')\n",
        "    BATCH_SIZE = 32\n",
        "    EMBEDDING_SIZE = 650\n",
        "    MAX_VOCAB_SIZE = 50000\n",
        "    TRAIN_DATA_SET = \"lm-train.txt\"\n",
        "    DEV_DATA_SET = \"lm-dev.txt\"\n",
        "    TEST_DATA_SET = \"lm-test.txt\"\n",
        "    BPTT_LENGTH = 32\n",
        "\n",
        "# For uploading data to Colab see, e.g., \n",
        "# https://medium.com/@philipplies/transferring-data-from-google-drive-to-google-cloud-storage-using-google-colab-96e088a8c041    \n",
        "#COLAB = False\n",
        "COLAB = True\n",
        "if COLAB:\n",
        "    from google.colab import drive \n",
        "    drive.mount('/content/gdrive')\n",
        "    PATH = \"gdrive/My Drive/\"\n",
        "else:\n",
        "    PATH = \"/Users/amitabh/mlpp22/Homework/hw4/\"\n",
        "    \n",
        "    \n",
        "LOG_FILE = \"language-model.log\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQIs9OK2BBkj"
      },
      "source": [
        "### Preprocessing using the legacy component of TorchText\n",
        "\n",
        "For our preprocessing we'll use the legacy component in TorchText version 0.11.2.  [Documentation](https://torchtext.readthedocs.io/en/latest/index.html) for this legacy component torchtext is relatively sparse (and, unfortunately, not very clear), but [Ben Trevett](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb) has a useful tutorial.  (If you are keen to understand this component, you may also want to look at the [source code](https://github.com/pytorch/text/tree/master/torchtext/legacy).)\n",
        "\n",
        "All the **legacy torchtext code is already provided**. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe2FRmolBBkn",
        "outputId": "9a958d5c-1c51-403a-9ac3-1f7abead2a6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 50002\n"
          ]
        }
      ],
      "source": [
        "TEXT = torchtext.legacy.data.Field(lower=True)\n",
        "\n",
        "train, val, test = torchtext.legacy.datasets.LanguageModelingDataset.splits(path=PATH, \n",
        "    train=TRAIN_DATA_SET, validation=DEV_DATA_SET, test=TEST_DATA_SET, text_field=TEXT)\n",
        "\n",
        "TEXT.build_vocab(train, max_size=MAX_VOCAB_SIZE)\n",
        "VOCAB_SIZE = len(TEXT.vocab)\n",
        "\n",
        "print(f'Vocabulary size: {VOCAB_SIZE}')\n",
        "\n",
        "train_iter, val_iter, test_iter = torchtext.legacy.data.BPTTIterator.splits(\n",
        "    (train, val, test), batch_size=BATCH_SIZE, device=DEVICE, bptt_len=BPTT_LENGTH, \n",
        "    repeat=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y7Us1voBBkr"
      },
      "source": [
        "### Back propagation through time (BPTT) iterator\n",
        "\n",
        "The [BPTTIterator](https://torchtext.readthedocs.io/en/latest/data.html#bpttiterator) is a custom torchtext iterator for language modeling using RNNs.  Suppose the text in an example is \"the quick brown fox\".  The target in the transducer-based RNN language model would then be \"quick brown fox jumps\".  This allows every prefix of the text to be used as an training example, with the corresponding word in the target text as the target word.  So the above would lead to four examples, written as text sequence -> target word:\n",
        "* \"the\" -> \"quick\"\n",
        "* \"the quick\" -> \"brown\"\n",
        "* \"the quick brown\" -> \"fox\"\n",
        "* \"the quick brown fox\" -> \"jump\"\n",
        "\n",
        "(Unlike some of the examples in class, here we treat words as part of a sequence without special consideration for sentences.  In particular, we don't use start/end of setence tags.)\n",
        "\n",
        "One very **significant feature** of the BPTTIterator is that examples continue across batches.  To illustrate let the original data be one long seqence $w_1, w_2, \\ldots, w_N$, in which, say, $N = 4,000$.  Further let each batch consist of $4$ examples, each of length 8.  Then the first batch created by BPTTIterator would be the following 4 examples---\n",
        "\n",
        "- $(w_1, w_2, \\ldots, w_{8}), (w_{1001}, w_{1002}, \\ldots, w_{1008}), \\ldots, (w_{3001}, w_{3002}, \\ldots, w_{3008}).$ \n",
        "\n",
        "and the second batch would be---\n",
        "\n",
        "- $(w_{9}, w_{10}, \\ldots, w_{16}), (w_{1009}, w_{1010}, \\ldots, w_{1016}), \\ldots, (w_{3009}, w_{3010}, \\ldots, w_{3016}).$\n",
        "\n",
        "This has implications on how the hidden state of the RNN is set for the second batch onwards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqBOF32nBBks",
        "outputId": "635ece75-f99c-4980-d302-e29b8f896db6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first three text/target sequences from the first batch are:\n",
            "\n",
            "     Text Sequence 0: anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans <unk> of the french revolution whilst the term\n",
            "     Target Sequence 0: originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans <unk> of the french revolution whilst the term is\n",
            "\n",
            "     Text Sequence 1: of natural history albert einstein the albert einstein institution the economist one zero zero years of einstein einstein home distributed computing project searching for gravitational waves predicted by einstein s theories world\n",
            "     Target Sequence 1: natural history albert einstein the albert einstein institution the economist one zero zero years of einstein einstein home distributed computing project searching for gravitational waves predicted by einstein s theories world year\n",
            "\n",
            "     Text Sequence 2: one eight eight two he set up a practice in plymouth he achieved his doctorate in one eight eight five his medical practice was unsuccessful while waiting for patients he began writing\n",
            "     Target Sequence 2: eight eight two he set up a practice in plymouth he achieved his doctorate in one eight eight five his medical practice was unsuccessful while waiting for patients he began writing stories\n",
            "\n",
            "Each sequence has BPTT_LENGTH = 32.\n",
            "\n",
            "Also the sequences continue in the next batch!\n",
            "\n",
            "     Text Sequence 0: is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by\n",
            "     Target Sequence 0: still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self\n",
            "\n",
            "     Text Sequence 1: year of physics two zero zero five a celebration of einstein s miracle year einstein year two zero zero five the guardian einstein s pacifist dilemma revealed einstein s theory of relativity\n",
            "     Target Sequence 1: of physics two zero zero five a celebration of einstein s miracle year einstein year two zero zero five the guardian einstein s pacifist dilemma revealed einstein s theory of relativity in\n",
            "\n",
            "     Text Sequence 2: stories his first literary experience came in chambers s edinburgh journal before he was two zero it was only after he subsequently moved his practice to <unk> that he began to indulge\n",
            "     Target Sequence 2: his first literary experience came in chambers s edinburgh journal before he was two zero it was only after he subsequently moved his practice to <unk> that he began to indulge more\n",
            "\n"
          ]
        }
      ],
      "source": [
        "it = iter(train_iter)\n",
        "batch = next(it)\n",
        "print(\"The first three text/target sequences from the first batch are:\\n\")\n",
        "indent = \" \" * 4\n",
        "for j in range(3):\n",
        "    print(indent, f\"Text Sequence {j}:\", \n",
        "          \" \".join([TEXT.vocab.itos[i] for i in batch.text[:,j].data]))\n",
        "    print(indent, f\"Target Sequence {j}:\",\n",
        "          \" \".join([TEXT.vocab.itos[i] for i in batch.target[:,j].data]))\n",
        "    print()\n",
        " \n",
        "print(f\"Each sequence has BPTT_LENGTH = {BPTT_LENGTH}.\\n\")\n",
        "print(\"Also the sequences continue in the next batch!\\n\")\n",
        "batch = next(it)\n",
        "for j in range(3):\n",
        "    print(indent, f\"Text Sequence {j}:\", \n",
        "          \" \".join([TEXT.vocab.itos[i] for i in batch.text[:,j].data]))\n",
        "    print(indent, f\"Target Sequence {j}:\",\n",
        "          \" \".join([TEXT.vocab.itos[i] for i in batch.target[:,j].data]))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smzi0zXzBBkv"
      },
      "source": [
        "#### Initializing hidden vectors from the detached hidden vectors of previous batch\n",
        "\n",
        "Since sequences continue across batches, for proper training, **the final output hidden vectors in a batch should be used to initialize the hidden vectors for the next batch**.  But care should be taken to detach vectors used for initialization from the computational graph, else gradients would flow \"from one batch to the previous\" and training would be increasingly slow. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B853z05BBkv"
      },
      "source": [
        "### Define the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uge8v-wXj4Q"
      },
      "source": [
        "\n",
        "Our RNN based language model (when using an LSTM) for a language model is as follows:\n",
        "- Let the input sequence---the *context*---be $w_1, w_2, \\ldots, w_n$, and let the target sequence be $w_2, \\ldots, w_n, w_{n+1}$.\n",
        "- At step $i$ of the input, for $1 \\leq i \\leq n$:\n",
        "    - $x_i = E_{[w_i]}$.\n",
        "    - $y_i, (h_i, c_i) = \\text{LSTM}(x_i, (h_{i-1}, c_{i-1}))$.  For LSTMs, $y_i$ equals $h_i$.\n",
        "    - $\\widehat{y_i} = \\text{softmax}(y_i W + b)$ in which $\\widehat{y_i}$ is the predicted probability distribution for $w_{i+1}$.\n",
        "    - In the above \n",
        "        - $x_i$ is $1 \\times \\text{embedding dim}$ \n",
        "        - $y_i$, $h_i$ and $c_i$ are $1 \\times \\text{hidden dim}$\n",
        "        - $\\widehat{y}_i$ is $1 \\times \\text{vocab size}$.\n",
        "- The loss $\\ell = \\sum_{i=1}^n \\log \\widehat{y}_{i_{[w_{i+1}]}}$, in which $\\log \\widehat{y}_{i_{[w_{i+1}]}}$ is the component of $\\log \\widehat{y}_{i}$ corresponding to the element $w_{i+1}$.\n",
        "\n",
        "Since the sequences continue across batches we retain the hidden states across batches. Specifically, consider the $k$th example in batch $j$.  For $j=1$, i.e., first batch, the corresponding $(h_0, c_0)$ for the $k$th example is set to all zeros.  But for $j > 1$, the corresponding $(h_0, c_0)$ is set to $(h_{n}, c_{n})$ of the $k$th example in batch $j-1$.\n",
        "\n",
        "In PyTorch we do not call the forward function separately for each step $i$.  Instead we call the model with\n",
        "\n",
        "- tensors corresponding to $(w_1, w_2, \\ldots, w_n)$ and $(h_0, c_0)$\n",
        "\n",
        "and receive as ouput\n",
        "\n",
        "- $(y_1, y_2, \\ldots, y_n)$ and $(h_n, c_n)$.\n",
        "\n",
        "Further the above is combined for several examples into one batch.  Please read the PyTorch documentation to learn more about building models\n",
        "            with RNNs.  E.g., see the documentation on [LSTMs](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) and [Robert Gutherie's Tutorial](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#sphx-glr-beginner-nlp-sequence-models-tutorial-py) on working with LSTMs.\n",
        "            \n",
        "The above can be adapted easily to [GRUs](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html) or [Simple RNNs](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) since the PyTorch interface is very similar.\n",
        "\n",
        "**Task 1** [20 points]: Complete the code for the class `RNNLM` based on the description above.  (Some extra parameters are provided since in a later task, you'll modify your code to incorporate the following: (i) replace the LSTM with a GRU or a Simple RNN, (ii) increase the number of LSTM layers, and (iii) add dropout.) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "beflzeEkBBkw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class RNNLM(nn.Module):\n",
        "    \"\"\" Container module with an linear encoder/embedding, an RNN module, and a linear decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, rnn_type, vocab_size, embedding_dim, hidden_dim, num_layers, \n",
        "                 dropout=0.5):\n",
        "        ''' Initialize model parameters corresponding to ---\n",
        "            - embedding layer\n",
        "            - recurrent neural network layer---one of LSTM, GRU, or RNN---with \n",
        "              optionally more than one layer\n",
        "            - linear layer to map from hidden vector to the vocabulary\n",
        "            - optionally, dropout layers.  Dropout layers can be placed after \n",
        "              the embedding layer or/and after the RNN layer. Dropout within\n",
        "              an RNN is only applied when there are two or more num_layers.\n",
        "            - optionally, initialize the model parameters.\n",
        "            \n",
        "            The arguments are:\n",
        "            \n",
        "            rnn_type: One of 'LSTM', 'GRU', 'RNN_TANH', 'RNN_RELU'\n",
        "            vocab_size: size of vocabulary\n",
        "            embedding_dim: size of an embedding vector\n",
        "            hidden_dim: size of hidden/state vector in RNN\n",
        "            num_layers: number of layers in RNN\n",
        "            dropout: dropout probability.\n",
        "            \n",
        "        '''\n",
        "        super(RNNLM, self).__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        if rnn_type == \"LSTM\":\n",
        "          self.lstm = nn.LSTM(input_size=embedding_dim,\n",
        "                            hidden_size=hidden_dim,\n",
        "                            num_layers = num_layers)\n",
        "        elif rnn_type == \"GRU\":\n",
        "          self.GRU = nn.GRU(input_size=embedding_dim,\n",
        "                            hidden_size=hidden_dim,\n",
        "                            num_layers = num_layers)\n",
        "        elif rnn_type == \"RNN_TANH\":\n",
        "          self.RNN_TANH = nn.RNN(input_size=embedding_dim,\n",
        "                            hidden_size=hidden_dim,\n",
        "                            num_layers = num_layers,\n",
        "                            nonlinearity = \"tanh\")\n",
        "        elif rnn_type == \"RNN_RELU\":\n",
        "          self.RNN_RELU =nn.RNN(input_size=embedding_dim,\n",
        "                             hidden_size=hidden_dim,\n",
        "                            num_layers = num_layers,\n",
        "                            nonlinearity = \"relu\")\n",
        "        else:\n",
        "          assert \"Incorrect RNN Type.\"\n",
        "        \n",
        "        self.dropout = nn.Dropout(p = dropout)\n",
        "        self.decoder = nn.Linear(hidden_dim, vocab_size) #vocab_size?\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.n_layers = num_layers\n",
        "        self.nhid = hidden_dim\n",
        "\n",
        "    def forward(self, input, hidden0):\n",
        "        ''' \n",
        "        Run forward propagation for a given minibatch of inputs using\n",
        "        hidden0 as the initial hidden state.\n",
        "\n",
        "        In LSTMs hidden0 = (h_0, c_0). \n",
        "\n",
        "        The output of the RNN includes the hidden vector hiddenn = (h_n, c_n).\n",
        "        Return this as well so that it can be used to initialize the next\n",
        "        batch.\n",
        "        \n",
        "        Unlike previous homework sets do not apply softmax or logsoftmax here, since we'll use\n",
        "        the more efficient CrossEntropyLoss.  See \n",
        "        https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html.\n",
        "        '''\n",
        "        encoded = self.embedding(input.long())\n",
        "        encoded = self.dropout(encoded)\n",
        "        lstm_out, hidden = self.lstm(encoded, hidden0)\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "        decoded = self.decoder(lstm_out)\n",
        "        decoded = torch.squeeze(decoded)\n",
        "\n",
        "        \n",
        "        return decoded, hidden\n",
        "        \n",
        "    def init_hidden(self, bsz):\n",
        "        return (torch.zeros(self.n_layers, bsz, self.embedding_dim),\n",
        "            torch.zeros(self.n_layers, bsz, self.embedding_dim))\n",
        "        \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOu67_BOBBky"
      },
      "source": [
        "### Evaluate on a given data set\n",
        "\n",
        "The function for evaluation is provided below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Po5NsbTOBBkz"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data):\n",
        "    '''\n",
        "    Evaluate the model on the given data.\n",
        "    '''\n",
        "  \n",
        "    model.eval()\n",
        "    it = iter(data)\n",
        "    total_count = 0. # Number of target words seen\n",
        "    total_loss = 0. # Loss over all target words\n",
        "    with torch.no_grad():\n",
        "        # No gradients need to be maintained during evaluation\n",
        "        # There are no hidden tensors for the first batch, and so will default to zeros.\n",
        "        hidden = None \n",
        "        for i, batch in enumerate(it):\n",
        "            ''' Do the following:\n",
        "                - Extract the text and target from the batch, and if using CUDA (essentially, using GPUs), place \n",
        "                  the tensors on cuda, using a commands such as \"text = text.cuda()\".  More details are at\n",
        "                  https://pytorch.org/docs/stable/notes/cuda.html.\n",
        "                - Pass the hidden state vector from output of previous batch as the initial hidden vector for\n",
        "                  the current batch. \n",
        "                - Call forward propagation to get output and final hidden state vector.\n",
        "                - Compute the cross entropy loss\n",
        "                - The loss_fn computes the average loss per target word in the batch.  Count the number of target\n",
        "                  words in the batch (it is usually the same, except for the last batch), and use it to track the \n",
        "                  total count (of target words) and total loss see so far over all batches.\n",
        "            '''\n",
        "            text, target = batch.text, batch.target\n",
        "            if USE_CUDA:\n",
        "                text, target = text.cuda(), target.cuda()\n",
        "            if target.shape[0] != 10: #kicking out the last batch\n",
        "              output, hidden = model(text, hidden)\n",
        "              loss = loss_fn(output.view(-1, output.size(-1)), target.view(-1))\n",
        "              total_count += np.multiply(*text.size())\n",
        "              total_loss += loss.item()*np.multiply(*text.size())\n",
        "                \n",
        "    loss = total_loss / total_count\n",
        "    model.train()\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcg9tUbNOEbg"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Training the model is mostly similar to previous homework sets except for:\n",
        "- A detached hidden vector is applied to the second batch onwards as described above.\n",
        "- Every, say, 10,000 iterations evaluate the model on a validation set, and if the mean loss is the lowest so far, save a copy of it.  After training, this \"best model\" is used for testing. \n",
        "      \n",
        "**Task 2** [30]: Complete the code below for training the model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, data, i, optimizer, hidden = None, USE_CUDA = True, print_every = 250):\n",
        "  start_time = time.time()\n",
        "  total_loss = 0\n",
        "  text, target = data.text, data.target\n",
        "  if USE_CUDA:\n",
        "      text, target = text.cuda(), target.cuda()\n",
        "  hidden = repackage_hidden(hidden)\n",
        "  optimizer.zero_grad()\n",
        "  model.zero_grad()\n",
        "  output, hidden = model(text, hidden)\n",
        "\n",
        "  if output.shape[0] == target.shape[0]:\n",
        "    loss = loss_fn(output.view(-1, output.size(-1)), target.view(-1))\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "  if (i + 1) % print_every == 0:\n",
        "      cur_loss = total_loss / print_every\n",
        "      elapsed = time.time() - start_time\n",
        "      print(f\"Average loss so far: {round(cur_loss, 2)} | Batch {i+1} | Time {round(elapsed, 2)}\")\n",
        "      total_loss = 0\n",
        "      start_time = time.time()"
      ],
      "metadata": {
        "id": "Vk_JyF-WvDcR"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "8dlLJ5FTBBk1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdbdb7f9-7883-4caf-a987-94abca783a11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " #### EPOCH 0 #####\n",
            "Average loss so far: 0.03 | Batch 250 | Time 0.1\n",
            "Average loss so far: 0.03 | Batch 500 | Time 0.11\n",
            "Average loss so far: 0.03 | Batch 750 | Time 0.1\n",
            "Average loss so far: 0.03 | Batch 1000 | Time 0.11\n",
            "Average val loss 6.4184726944888935\n",
            "New best model saved!\n",
            "Average loss so far: 0.03 | Batch 1250 | Time 0.11\n",
            "Average loss so far: 0.03 | Batch 1500 | Time 0.11\n",
            "Average loss so far: 0.03 | Batch 1750 | Time 0.11\n",
            "Average loss so far: 0.03 | Batch 2000 | Time 0.11\n",
            "Average val loss 6.294022049674069\n",
            "New best model saved!\n",
            "Average loss so far: 0.03 | Batch 2250 | Time 0.11\n",
            "Average loss so far: 0.03 | Batch 2500 | Time 0.11\n",
            "Average loss so far: 0.02 | Batch 2750 | Time 0.11\n",
            "Average loss so far: 0.03 | Batch 3000 | Time 0.11\n",
            "Average val loss 6.200891359742866\n",
            "New best model saved!\n",
            "Average loss so far: 0.03 | Batch 3250 | Time 0.11\n",
            "Average loss so far: 0.02 | Batch 3500 | Time 0.11\n",
            "Average loss so far: 0.03 | Batch 3750 | Time 0.11\n",
            "Average loss so far: 0.03 | Batch 4000 | Time 0.11\n",
            "Average val loss 6.130427093103708\n",
            "New best model saved!\n",
            "Average loss so far: 0.02 | Batch 4250 | Time 0.11\n",
            "Average loss so far: 0.02 | Batch 4500 | Time 0.11\n",
            "Average loss so far: 0.02 | Batch 4750 | Time 0.11\n",
            "Average loss so far: 0.03 | Batch 5000 | Time 0.11\n",
            "Average val loss 6.073358086390668\n",
            "New best model saved!\n",
            "Average loss so far: 0.02 | Batch 5250 | Time 0.11\n",
            "Average loss so far: 0.02 | Batch 5500 | Time 0.11\n",
            "Average loss so far: 0.03 | Batch 5750 | Time 0.11\n",
            "Average loss so far: 0.02 | Batch 6000 | Time 0.11\n",
            "Average val loss 6.0234286697992845\n",
            "New best model saved!\n",
            "Average loss so far: 0.02 | Batch 6250 | Time 0.11\n",
            "Average loss so far: 0.02 | Batch 6500 | Time 0.11\n",
            "Average loss so far: 0.02 | Batch 6750 | Time 0.11\n",
            "Average loss so far: 0.02 | Batch 7000 | Time 0.11\n",
            "Average val loss 5.977101130575815\n",
            "New best model saved!\n",
            "Average loss so far: 0.02 | Batch 7250 | Time 0.11\n",
            "Average loss so far: 0.02 | Batch 7500 | Time 0.11\n",
            " #### EPOCH 1 #####\n"
          ]
        }
      ],
      "source": [
        "GRAD_CLIP = 1.\n",
        "NUM_EPOCHS = 2\n",
        "LOG_INTERVAL = 100\n",
        "import os\n",
        "SAVE_BEST = os.path.join(PATH, 'model.pt')\n",
        "import time\n",
        "import math\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if h is None:\n",
        "        return None\n",
        "    elif isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "model = RNNLM(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, dropout=0.5)\n",
        "if USE_CUDA:\n",
        "    model = model.cuda()\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss() ## Used instead of NLLLoss.\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "val_losses = []\n",
        "best_model = None\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\"\" #### EPOCH {epoch} #####\"\"\")\n",
        "    model.train()\n",
        "    # There are no hidden tensors for the first batch, and so will default to zeros.\n",
        "    hidden = None\n",
        "    min_val_loss = math.inf\n",
        "    best_model = None\n",
        "    for i, batch in enumerate(it):\n",
        "      train_model(model, batch, i, optimizer = optimizer)\n",
        "  \n",
        "      if (i + 1) % 1000 == 0:\n",
        "        val_loss = evaluate(model, val_iter)\n",
        "        \n",
        "        val_losses.append(val_loss.item())\n",
        "        min_val_loss = min(min_val_loss, val_loss.item())\n",
        "        print(f\"Average val loss {sum(val_losses) / len(val_losses)}\")\n",
        "        if min_val_loss == val_loss.item():\n",
        "          with open(SAVE_BEST, 'wb') as f:\n",
        "            torch.save(model, f)\n",
        "          print(\"New best model saved!\")\n",
        "\n",
        "        \n",
        "       \n",
        "\n",
        "    ''' Do the following:\n",
        "      \n",
        "        - Pass the hidden state vector from output of previous batch as the initial hidden vector for\n",
        "          the current batch. But detach each tensor in the hidden state vector using tensor.detach() or\n",
        "          the provided repackage_hidden(). See\n",
        "          https://pytorch.org/docs/master/generated/torch.Tensor.detach_.html#torch-tensor-detach\n",
        "        - Zero out the model gradients to reset backpropagation for current batch\n",
        "        - Call forward propagation to get output and final hidden state vector.\n",
        "        - Compute the cross entropy loss\n",
        "        - Run back propagation to set the gradients for each model parameter.\n",
        "        - Clip the gradients that may have exploded. See Sec 5.2.4 in the Goldberg textbook, and\n",
        "          https://pytorch.org/docs/master/generated/torch.nn.utils.clip_grad_norm_.html#torch-nn-utils-clip-grad-norm\n",
        "        - Run a step of gradient descent. \n",
        "        - Print the batch loss after every few iterations. (Say every 100 when developing, every 1000 otherwise.)\n",
        "        - Evaluate your model on the validation set after every, say, 10000 iterations and save it to val_losses. If\n",
        "          your model has the lowest validation loss so far, copy it to best_model. For that it is recommended that\n",
        "          copy the state_dict rather than use deepcopy, since the latter doesn't work on Colab.  See discussion at \n",
        "          https://discuss.pytorch.org/t/deep-copying-pytorch-modules/13514. This is Early Stopping and is described\n",
        "          in Sec 2.3.1 of Lecture notes by Cho: \n",
        "          https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf\n",
        "    '''\n",
        "    \n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "SAVE_BEST = os.path.join(PATH, 'model.pt')\n",
        "\n",
        "with open(SAVE_BEST, 'rb') as f:\n",
        "    best_model = torch.load(f)\n",
        "    # After loading the RNN params, they are not a continuous chunk of memory.\n",
        "    # flatten_paramters() makes them a continuous chunk, and will speed up the forward pass.\n",
        "    # Currently, only RNN model supports flatten_parameters function.\n",
        "    best_model.lstm.flatten_parameters()\n"
      ],
      "metadata": {
        "id": "3T6KhW2cGVuP"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "x3ooK74UBBk4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c32d4fe1-7e79-4a57-f41c-231445ef8d6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity:  298.60925976836154\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Evaluate the loss of best_model on the validation set and compute its perplexity.\n",
        "'''\n",
        "## load best model\n",
        "with open(SAVE_BEST, 'rb') as f:\n",
        "    best_model = torch.load(f)\n",
        "    # After loading the RNN params, they are not a continuous chunk of memory.\n",
        "    # flatten_paramters() makes them a continuous chunk, and will speed up the forward pass.\n",
        "    # Currently, only RNN model supports flatten_parameters function.\n",
        "    best_model.lstm.flatten_parameters()\n",
        "\n",
        "best_model_set = RNNLM(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, dropout=0.5)\n",
        "\n",
        "\n",
        "val_loss = evaluate(best_model, val_iter)\n",
        "print(\"perplexity: \", np.exp(val_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6klWxMwBBk7"
      },
      "source": [
        "### Use the best model to evaluate the test dataset. \n",
        "\n",
        "We expect a test perplexity of less than 250 on the full model after a couple of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "oE7CK7XxBBk7",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc9abef9-2607-42a4-d6e8-a90f0aac7a67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity:  347.02976208615513\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Evaluate the loss of best_model on the test set and compute its perplexity.\n",
        "'''\n",
        "test_loss = evaluate(best_model, test_iter)\n",
        "print(\"perplexity: \", np.exp(test_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ7-S1FuBBk9"
      },
      "source": [
        "### Use the model to generate some sentences\n",
        "\n",
        "**Task 3** [20]: Write code to generate random sentences.  Section 9.5 in the Goldberg textbook describes how this can be done.  Since we don't have a start symbol, for the first word simply pick a random word from the vocabulary.\n",
        "\n",
        "You'll notice that the full sequences don't make much sense, but subsequences sound reasonably correct. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "LNSlzc-8BBk-"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Use the model to generate 5 random sequences of length 50 each.\n",
        "'''\n",
        "def generate_text(sampling_func, model = best_model):\n",
        "    # # Generation with LSTM lm given a sampling function and a prompt\n",
        "    prompt = random.choice(TEXT.vocab.itos)\n",
        "    id_word = TEXT.vocab.itos.index(prompt)\n",
        "    max_length = 50\n",
        "    hidden = model.init_hidden(1)\n",
        "    hidden = repackage_hidden(hidden)\n",
        "    if USE_CUDA:\n",
        "      hidden = (hidden[0].to('cuda'), hidden[1].to('cuda'))\n",
        "    with torch.no_grad():  # no tracking history\n",
        "        input = torch.tensor(id_word).to(\"cuda\")\n",
        "        output, hidden = model(torch.tensor([[id_word]]).to(\"cuda\"), hidden)\n",
        "        word_prob = torch.nn.functional.softmax(output, dim=0).cpu()\n",
        "        generations = []\n",
        "        for i in range(max_length):\n",
        "            word_idx = sampling_func(word_prob)\n",
        "            word = TEXT.vocab.itos[word_idx]\n",
        "            if word != \"<unk>\":\n",
        "              generations.append(word)\n",
        "            if word == \"<eos>\":\n",
        "                break\n",
        "            new_word = torch.LongTensor([[word_idx]]).to(\"cuda\")\n",
        "            output, hidden = model(new_word, hidden)\n",
        "            word_prob = torch.nn.functional.softmax(output, dim=0).cpu()\n",
        "    return generations\n",
        "\n",
        "def topk_sampling_5(word_prob):\n",
        "    k = 5\n",
        "    topk = torch.topk(word_prob.flatten(), k)\n",
        "    values = topk.values / topk.values.sum()\n",
        "    indices = topk.indices\n",
        "    index = torch.multinomial(values, 1).item()\n",
        "    word_id = list(indices)[index]\n",
        "    return word_id"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  generations = generate_text(topk_sampling_5) # replace sample_func with the sampling function that you would like to try\n",
        "  print('prompt: ' + \" \".join(generations))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdJVWKsAuJ9Y",
        "outputId": "9bff3226-7ad8-465d-f755-b561ea268558"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: the city was the most widely known part of the united kingdom and the united kingdom in one nine four four and one nine nine four in one nine nine nine the first president of the united states were named after the soviet union the of the was\n",
            "prompt: and in one nine nine seven the of the and the of the of the and the one nine nine zero s and the first year one nine four seven was the first president and the of the one eight seven\n",
            "prompt: of the and and the of in the early the of and the of is the only part for its first day in the one nine six zero s the first part of the united kingdom and a major government\n",
            "prompt: in the united states in one seven zero zero the of the of the and and in the one eight th century one seven five zero was an of in a few years of the city s and the\n",
            "prompt: and the new england and the north east the river is the largest and other in a and the one eight th century as the president of the the of a one eight nine four one nine nine nine isbn zero zero three\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEqN7I4xOEbi"
      },
      "source": [
        "### Choose the best sentence from alternatives\n",
        "\n",
        "Generating random sentences as above is, however, not the objective of a language model.  Rather it is used as an auxiliary tool to choose the best sequence given some choices by comparing their perplexities.\n",
        "\n",
        "**Task 4** [5]: Use the code below to compute perplexities of the given six sentences.  Discuss the model's performance in choosing the best alternative.  (The code uses TorchText functions which are designed for much larger datasets.  So the perplexities below are approximate. Even so they illustrate the usefullness of our model.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "UvIWKlt1OEbj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1e43e74-a6b6-4ece-c9c2-1151d8412c66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early in the pandemic, there was hope that the world would one day achieve herd immunity, the point when the coronavirus lacks hosts to spread easily. But over a year later, the virus is crushing India with a fearsome second wave and surging in countries from Asia to Latin America.\n",
            "perplexity:  3126.4074563268705\n",
            "\n",
            "Early in the pandemic, there was hope that the world would one day achieve herd immunity, the point when the coronavirus lacks hosts to spread easily. But over a year later, the virus is dancing India with a fearsome second wave and surging in countries from Asia to Latin America.\n",
            "perplexity:  3026.8301228884525\n",
            "\n",
            "Early in the pandemic, there was hope that the world would one day achieve herd immunity, the point when the coronavirus lacks hosts to spread easily. But over a year later, the virus is run India with a fearsome second wave and surging in countries from Asia to Latin America.\n",
            "perplexity:  3274.631956383099\n",
            "\n",
            "Early in the pandemic, there was hope that the cat  would one day achieve herd immunity, the point when the coronavirus lacks hosts to spread easily. But over a year later, the virus isrun India with a fearsome second wave and surging in countries from Asia to Latin America.\n",
            "perplexity:  6396.16862190239\n",
            "\n",
            "easily. wave a crushing year pandemic, day surging world that second when a America. from the spread the over Asia But was virus would with India there is immunity, achieve the and lacks Latin hope the one to coronavirus point Early to the fearsome later, in countries hosts in herd\n",
            "perplexity:  10610.111295710944\n",
            "\n",
            "Early in the pandemic Early in the pandemic Early in the pandemic Early in the pandemic Early in the pandemic Early in the pandemic Early in the pandemic Early in the pandemic\n",
            "perplexity:  442107.60697344615\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sen1 = (\"Early in the pandemic, there was hope that the world would one day achieve herd immunity, \"\n",
        "\"the point when the coronavirus lacks hosts to spread easily. But over a year later, the virus is \" \n",
        "\"crushing \"\n",
        "\"India with a fearsome second wave and surging in countries from Asia to Latin America.\")\n",
        "\n",
        "sen2 = (\"Early in the pandemic, there was hope that the world would one day achieve herd immunity, \"\n",
        "\"the point when the coronavirus lacks hosts to spread easily. But over a year later, the virus is \"\n",
        "\"dancing \"\n",
        "\"India with a fearsome second wave and surging in countries from Asia to Latin America.\")\n",
        "\n",
        "sen3 = (\"Early in the pandemic, there was hope that the world would one day achieve herd immunity, \"\n",
        "\"the point when the coronavirus lacks hosts to spread easily. But over a year later, the virus is \" \n",
        "\"run \"\n",
        "\"India with a fearsome second wave and surging in countries from Asia to Latin America.\")\n",
        "\n",
        "sen4 = (\"Early in the pandemic, there was hope that the \"\n",
        "\"cat \"\n",
        "\" would one day achieve herd immunity, \"\n",
        "\"the point when the coronavirus lacks hosts to spread easily. But over a year later, the virus is\"\n",
        "\"run \"\n",
        "\"India with a fearsome second wave and surging in countries from Asia to Latin America.\")\n",
        "\n",
        "sen5 = sen1.split()\n",
        "random.shuffle(sen5)\n",
        "sen5 = \" \".join(sen5)\n",
        "\n",
        "sen6 = \" \".join(['Early in the pandemic']*8)\n",
        "\n",
        "sen_list = [sen1, sen2, sen3, sen4, sen5, sen6]\n",
        "\n",
        "for sen in sen_list:\n",
        "\n",
        "    print(sen)\n",
        "    with open(PATH + \"temp_sentence.txt\", 'w') as text_file:\n",
        "        print(sen, file = text_file)\n",
        "\n",
        "    temp_ds = torchtext.legacy.datasets.LanguageModelingDataset(path=PATH + 'temp_sentence.txt', \n",
        "                                                                text_field=TEXT)\n",
        "\n",
        "\n",
        "    sen_iter = torchtext.legacy.data.BPTTIterator(temp_ds, batch_size=BATCH_SIZE, device=DEVICE, \n",
        "                                                  bptt_len=BPTT_LENGTH, repeat=False)\n",
        "        \n",
        "    sen_loss = evaluate(best_model, sen_iter)\n",
        "    print(\"perplexity: \", np.exp(sen_loss))\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sen_iter"
      ],
      "metadata": {
        "id": "W5tXzW1OLLZx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SzfKRtMBBlA"
      },
      "source": [
        "### Extensions\n",
        "\n",
        "**Task 5** [25]: Extend your model to incorporate the following options: (i) substitute the LSTM with a GRU or a Simple RNN, (ii) increase the number of LSTM layers, (iii) add dropout, (iv) add gradient clipping.  Report on the combination of these options which gives the best performance."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqgsHnQiOEbj"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "hw4_rnn_lang_model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}